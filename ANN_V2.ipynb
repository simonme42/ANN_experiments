{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANN_V2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMet+aBqKZ4Uc+Wm8SQ60zj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simonme42/ANN_experiments/blob/master/ANN_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkuCIknEWYLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import scipy\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "def sigmoid(Z):\n",
        "\n",
        "    A = 1/(1+np.exp(-Z))\n",
        "    cache = Z\n",
        "    \n",
        "    return A, cache\n",
        "\n",
        "def relu(Z):\n",
        "    \n",
        "    A = np.maximum(0,Z)\n",
        "    assert(A.shape == Z.shape)\n",
        "    cache = Z \n",
        "\n",
        "    return A, cache\n",
        "\n",
        "def softmax(Z):\n",
        "\n",
        "    T = (np.exp(Z))\n",
        "    A = T/np.sum(T)\n",
        "    cache = Z \n",
        "\n",
        "    return A, cache\n",
        "\n",
        "def relu_backward(dA, cache):\n",
        "    \n",
        "    Z = cache\n",
        "    dZ = np.array(dA, copy=True) # just converting dz to a correct object\n",
        "    \n",
        "    # When z <= 0, set dz to 0 \n",
        "    dZ[Z <= 0] = 0\n",
        "    \n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ\n",
        "\n",
        "def sigmoid_backward(dA, cache):\n",
        "    \n",
        "    Z = cache\n",
        "    \n",
        "    s = 1/(1+np.exp(-Z))\n",
        "    dZ = dA * s * (1-s)\n",
        "    \n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ\n",
        "\n",
        "def softmax_backward(dA, cache):\n",
        "    \n",
        "    Z = cache\n",
        "\n",
        "    S = softmax(Z)\n",
        "\n",
        "    # where i = j\n",
        "    div = S[0] * (1 - S[0])\n",
        "    div = S[1] * (1 - S[1])\n",
        "    div = S[2] * (1 - S[2])\n",
        "\n",
        "    # where i != j\n",
        "    div = 0 - (S[0] * S[1])\n",
        "    div = 0 - (S[1] * S[2])\n",
        "    div = 0 - (S[2] * S[3])\n",
        "\n",
        "    S_vector = S.reshape(S.shape[0],1)\n",
        "    S_matrix = np.tile(S_vector,S.shape[0])\n",
        "\n",
        "    dZ = np.diag(S) - (S_matrix * np.transpose(S_matrix))\n",
        "    \n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ\n",
        "\n",
        "def initialize_parameters_deep(layer_dims):\n",
        "    \n",
        "    #np.random.seed(1)\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)            # number of layers in the network\n",
        "\n",
        "    for l in range(1, L):\n",
        "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1]) #*0.01\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
        "        \n",
        "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
        "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
        "\n",
        "        \n",
        "    return parameters\n",
        "\n",
        "def linear_forward(A, W, b):\n",
        "    \n",
        "    Z = np.dot(W,A)+b\n",
        "    \n",
        "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
        "    cache = (A, W, b)\n",
        "    \n",
        "    return Z, cache\n",
        "\n",
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "    \n",
        "    if activation == \"sigmoid\":\n",
        "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = sigmoid(Z)\n",
        "    \n",
        "    elif activation == \"relu\":\n",
        "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = relu(Z)\n",
        "\n",
        "    elif activation == \"softmax\":\n",
        "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = softmax(Z)\n",
        "    \n",
        "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
        "    cache = (linear_cache, activation_cache)\n",
        "\n",
        "    return A, cache\n",
        "\n",
        "def L_model_forward(X, parameters, output_activation=\"sigmoid\"):\n",
        "\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) // 2                  # number of layers in the neural network\n",
        "    \n",
        "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
        "    for l in range(1, L):\n",
        "        A_prev = A \n",
        "        A, cache = linear_activation_forward(A_prev, parameters[\"W\"+str(l)], parameters[\"b\"+str(l)], activation=\"relu\")\n",
        "        caches.append(cache)\n",
        "    \n",
        "    if output_activation==\"sigmoid\":\n",
        "        # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
        "        AL, cache = linear_activation_forward(A, parameters[\"W\"+str(L)], parameters[\"b\"+str(L)], activation=\"sigmoid\")\n",
        "        caches.append(cache)\n",
        "\n",
        "    elif output_activation==\"softmax\":\n",
        "        AL, cache = linear_activation_forward(A, parameters[\"W\"+str(L)], parameters[\"b\"+str(L)], activation=\"softmax\")\n",
        "        caches.append(cache)\n",
        "    \n",
        "    assert(AL.shape == (1,X.shape[1]))\n",
        "            \n",
        "    return AL, caches\n",
        "\n",
        "def compute_cost(AL, Y, loss=\"crossentropy\"):\n",
        "    \n",
        "    m = Y.shape[1]\n",
        "\n",
        "    # Compute loss from aL and y.\n",
        "    if loss == \"crossentropy\":\n",
        "      logprobs = np.multiply(-np.log(AL),Y) + np.multiply(-np.log(1 - AL), 1 - Y)\n",
        "      cost_total =  np.sum(logprobs)\n",
        "    \n",
        "    elif loss == \"mse\":\n",
        "      cost_total = (np.sum((AL-Y)**2,axis=1, keepdims=True))\n",
        "    \n",
        "    cost_total = np.squeeze(cost_total)      # To make sure the cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
        "    assert(cost_total.shape == ())\n",
        "    \n",
        "    return cost_total\n",
        "\n",
        "def linear_backward(dZ, cache):\n",
        "\n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "    dW = 1./m * np.dot(dZ,A_prev.T)\n",
        "    db = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n",
        "    dA_prev = np.dot(W.T,dZ)\n",
        "    \n",
        "    assert (dA_prev.shape == A_prev.shape)\n",
        "    assert (dW.shape == W.shape)\n",
        "    assert (db.shape == b.shape)\n",
        "    \n",
        "    return dA_prev, dW, db\n",
        "\n",
        "def linear_activation_backward(dA, cache, activation):\n",
        "\n",
        "    linear_cache, activation_cache = cache\n",
        "    \n",
        "    if activation == \"relu\":\n",
        "        dZ = relu_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "        \n",
        "    elif activation == \"sigmoid\":\n",
        "        dZ = sigmoid_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "\n",
        "    elif activation == \"softmax\":\n",
        "        dZ = softmax_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "    \n",
        "    return dA_prev, dW, db\n",
        "\n",
        "def L_model_backward(AL, Y, caches, loss=\"crossentropy\"):\n",
        "\n",
        "    grads = {}\n",
        "    L = len(caches) # the number of layers\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
        "    \n",
        "    # Initializing the backpropagation\n",
        "\n",
        "    if loss == \"crossentropy\":\n",
        "      dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
        "    \n",
        "    elif loss == \"mse\":\n",
        "      dAL = 2*(AL-Y)\n",
        "    \n",
        "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
        "    current_cache = caches[L-1]\n",
        "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
        "    \n",
        "    for l in reversed(range(L-1)):\n",
        "        # lth layer: (RELU -> LINEAR) gradients.\n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = \"relu\")\n",
        "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "        grads[\"db\" + str(l + 1)] = db_temp\n",
        "\n",
        "    return grads\n",
        "\n",
        "def update_parameters(parameters, grads, learning_rate, decay=True, decay_param=0.95, epoch=0):\n",
        "    \n",
        "    L = len(parameters) // 2 # number of layers in the neural network\n",
        "\n",
        "    # Update rule for each parameter. Use a for loop.\n",
        "    for l in range(L):\n",
        "        if decay==True:\n",
        "          # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n",
        "          parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - ((decay_param**epoch)*learning_rate) * grads[\"dW\" + str(l+1)]\n",
        "          parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - ((decay_param**epoch)*learning_rate) * grads[\"db\" + str(l+1)]\n",
        "        elif decay==False:\n",
        "          # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n",
        "          parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
        "          parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
        "        \n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
        "        \n",
        "    return parameters\n",
        "\n",
        "def random_mini_batches(X, Y, mini_batch_size = 64):\n",
        "    \n",
        "    m = X.shape[1]                  # number of training examples\n",
        "    mini_batches = []\n",
        "        \n",
        "    # Step 1: Shuffle (X, Y)\n",
        "    permutation = list(np.random.permutation(m))\n",
        "    shuffled_X = X[:, permutation]\n",
        "    shuffled_Y = Y[:, permutation].reshape((1,m))\n",
        "\n",
        "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case\n",
        "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size\n",
        "    for k in range(0, num_complete_minibatches):\n",
        "        mini_batch_X = shuffled_X[:, mini_batch_size*k : mini_batch_size*(k+1)]\n",
        "        mini_batch_Y = shuffled_Y[:, mini_batch_size*k : mini_batch_size*(k+1)]\n",
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "    \n",
        "    # Handling the end case (last mini-batch < mini_batch_size)\n",
        "    if m % mini_batch_size != 0:\n",
        "        mini_batch_X = shuffled_X[:, mini_batch_size*(k+1):]\n",
        "        mini_batch_Y = shuffled_Y[:, mini_batch_size*(k+1):]\n",
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "    \n",
        "    return mini_batches\n",
        "\n",
        "def initialize_adam(parameters) :\n",
        "    \n",
        "    L = len(parameters) // 2 # number of layers in the neural networks\n",
        "    v = {}\n",
        "    s = {}\n",
        "    \n",
        "    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n",
        "    for l in range(L):\n",
        "        v[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)].shape[0], parameters[\"W\" + str(l+1)].shape[1]))\n",
        "        v[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)].shape[0], parameters[\"b\" + str(l+1)].shape[1]))\n",
        "        s[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)].shape[0], parameters[\"W\" + str(l+1)].shape[1]))\n",
        "        s[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)].shape[0], parameters[\"b\" + str(l+1)].shape[1]))\n",
        "    \n",
        "    return v, s\n",
        "\n",
        "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,beta1 = 0.9, \n",
        "                                beta2 = 0.999,  epsilon = 1e-8, decay=True, decay_param=0.95, epoch=0):\n",
        "    \n",
        "    L = len(parameters) // 2                 # number of layers in the neural networks\n",
        "    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n",
        "    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n",
        "    \n",
        "    # Perform Adam update on all parameters\n",
        "    for l in range(L):\n",
        "        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n",
        "        v[\"dW\" + str(l+1)] = beta1 * v[\"dW\" + str(l+1)] + (1-beta1) * grads['dW' + str(l+1)]\n",
        "        v[\"db\" + str(l+1)] = beta1 * v[\"db\" + str(l+1)] + (1-beta1) * grads['db' + str(l+1)]\n",
        "\n",
        "        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n",
        "        v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)] / (1-(beta1**t))\n",
        "        v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)] / (1-(beta1**t))\n",
        "\n",
        "        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n",
        "        s[\"dW\" + str(l+1)] = beta2 * s[\"dW\" + str(l+1)] + (1-beta2) * grads['dW' + str(l+1)]**2\n",
        "        s[\"db\" + str(l+1)] = beta2 * s[\"db\" + str(l+1)] + (1-beta2) * grads['db' + str(l+1)]**2\n",
        "\n",
        "        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n",
        "        s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)] / (1-(beta2**t))\n",
        "        s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)] / (1-(beta2**t))\n",
        "\n",
        "        if decay==True:\n",
        "          # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n",
        "          parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - ((decay_param**epoch)*learning_rate) * (v_corrected[\"dW\" + str(l+1)]/(np.sqrt(s_corrected[\"dW\" + str(l+1)])+epsilon))\n",
        "          parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - ((decay_param**epoch)*learning_rate) * (v_corrected[\"db\" + str(l+1)]/(np.sqrt(s_corrected[\"db\" + str(l+1)])+epsilon))\n",
        "        elif decay==False:\n",
        "          # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n",
        "          parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * (v_corrected[\"dW\" + str(l+1)]/(np.sqrt(s_corrected[\"dW\" + str(l+1)])+epsilon))\n",
        "          parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * (v_corrected[\"db\" + str(l+1)]/(np.sqrt(s_corrected[\"db\" + str(l+1)])+epsilon))\n",
        "    return parameters, v, s\n",
        "\n",
        "def predict(X, y, parameters, loss=\"crossentropy\", output_activation=\"sigmoid\"):\n",
        "\n",
        "    m = X.shape[1]\n",
        "    n = len(parameters) // 2 # number of layers in the neural network\n",
        "\n",
        "    if loss == \"crossentropy\":\n",
        "     \n",
        "      p = np.zeros((1,m))\n",
        "        \n",
        "      # Forward propagation\n",
        "      probas, caches = L_model_forward(X, parameters, output_activation)\n",
        "        \n",
        "      # convert probas to 0/1 predictions\n",
        "      for i in range(0, probas.shape[1]):\n",
        "          if probas[0,i] > 0.5:\n",
        "            p[0,i] = 1\n",
        "          else:\n",
        "            p[0,i] = 0\n",
        "      \n",
        "      print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
        "\n",
        "    elif loss == \"mse\":\n",
        "\n",
        "      p, caches = L_model_forward(X, parameters, output_activation)\n",
        "\n",
        "      print(\"Accuracy: \"  + str((1/m)*np.sum(p-y)**2))\n",
        "    \n",
        "    #print results\n",
        "    #print (\"predictions: \" + str(p))\n",
        "    #print (\"true labels: \" + str(y))\n",
        "    \n",
        "    return p\n",
        "\n",
        "def print_mislabeled_images(classes, X, y, p):\n",
        "\n",
        "    a = p + y\n",
        "    mislabeled_indices = np.asarray(np.where(a == 1))\n",
        "    plt.rcParams['figure.figsize'] = (40.0, 40.0) # set default size of plots\n",
        "    num_images = len(mislabeled_indices[0])\n",
        "    for i in range(num_images):\n",
        "        index = mislabeled_indices[1][i]\n",
        "        \n",
        "        plt.subplot(2, num_images, i + 1)\n",
        "        plt.imshow(X[:,index].reshape(64,64,3), interpolation='nearest')\n",
        "        plt.axis('off')\n",
        "        plt.title(\"Prediction: \" + classes[int(p[0,index])].decode(\"utf-8\") + \" \\n Class: \" + classes[y[0,index]].decode(\"utf-8\"))\n",
        "\n",
        "def train_model(X, Y, layers_dims, learning_rate = 0.0075, num_epochs = 3000, print_cost=True, \n",
        "                loss=\"crossentropy\", optimizer=\"GD\", mini_batch_size=64, decay=True, decay_param=0.95, output_activation=\"sigmoid\"):\n",
        "\n",
        "    L = len(layers_dims)             # number of layers in the neural networks\n",
        "    costs = []                       # to keep track of the cost\n",
        "    t = 0                            # initializing the counter required for Adam update\n",
        "    m = X.shape[1]                   # number of training examples\n",
        "\n",
        "    parameters = initialize_parameters_deep(layers_dims)\n",
        "\n",
        "    # Initialize the optimizer\n",
        "    if optimizer == \"GD\":\n",
        "        pass # no initialization required for gradient descent\n",
        "    elif optimizer == \"Adam\":\n",
        "        v, s = initialize_adam(parameters)\n",
        "    \n",
        "    # Optimization loop \n",
        "    for i in range(num_epochs):\n",
        "        # Define the random minibatches\n",
        "        minibatches = random_mini_batches(X, Y, mini_batch_size)\n",
        "        cost_total = 0\n",
        "\n",
        "        for minibatch in minibatches:\n",
        "            (minibatch_X, minibatch_Y) = minibatch\n",
        "\n",
        "            AL, caches = L_model_forward(minibatch_X, parameters, output_activation)\n",
        "            \n",
        "            # Compute cost\n",
        "            cost_total += compute_cost(AL, minibatch_Y, loss=loss)\n",
        "        \n",
        "            # Backward propagation\n",
        "            grads = L_model_backward(AL, minibatch_Y, caches, loss=loss)\n",
        "    \n",
        "            # Update parameters depending on optimization algorithm\n",
        "            if optimizer == \"GD\":\n",
        "                parameters = update_parameters(parameters, grads, learning_rate, decay=decay, decay_param=decay_param, epoch=i)\n",
        "            elif optimizer == \"Adam\":\n",
        "                t = t + 1 # Adam counter\n",
        "                parameters, v, s  = update_parameters_with_adam(parameters, grads, v, s, t, learning_rate, decay=decay, decay_param=decay_param, epoch=i)\n",
        "                    \n",
        "        cost_avg = cost_total / m\n",
        "        \n",
        "        # Print the cost every 100 epoch\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after epoch %i: %f\" %(i, cost_avg))\n",
        "            costs.append(cost_avg)          \n",
        "            \n",
        "    # plot the cost\n",
        "    print(cost_avg)\n",
        "    plt.plot(costs)\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('epochs (per 100)')\n",
        "    plt.title(\"Learning rate = \" + str(learning_rate))\n",
        "    plt.show()\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJPrIKAW7bij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxD6s--i2jL7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "outputId": "2f63ab7a-1504-4bae-df3d-af2a1db3a991"
      },
      "source": [
        "def kronecker_matrix(x,y):\n",
        "  kmat = np.meshgrid(x,x)[0] == np.meshgrid(y,y)[1]\n",
        "  return kmat.astype(np.int)   #kmat is a boolean, with .astype you pass it to an integer, if you like\n",
        "\n",
        "#example:\n",
        "kronecker_matrix(np.arange(5), np.arange(5))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-3319e88a4a1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#example:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mkronecker_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-3319e88a4a1c>\u001b[0m in \u001b[0;36mkronecker_matrix\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mkronecker_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mkmat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeshgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeshgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mkmat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m#kmat is a boolean, with .astype you pass it to an integer, if you like\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#example:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'bool' object has no attribute 'astype'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gU-1dU2QZFwY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Z = np.array([0,1,2,3]).reshape(4,1)\n",
        "T = (np.exp(Z))\n",
        "A = T/np.sum(T)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGG-przNWn_p",
        "colab_type": "code",
        "outputId": "f5570c56-bd29-45d3-f7c0-f685f9ce3ea2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "Z"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [1],\n",
              "       [2],\n",
              "       [3]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5a9W5DzZjup",
        "colab_type": "code",
        "outputId": "f9d8a226-b044-4223-f630-57715be56c80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "T"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.        ],\n",
              "       [ 2.71828183],\n",
              "       [ 7.3890561 ],\n",
              "       [20.08553692]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIgwxde2Zmpo",
        "colab_type": "code",
        "outputId": "93d226b3-bfce-4106-b2ba-1601b15b087b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "A"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.0320586 ],\n",
              "       [0.08714432],\n",
              "       [0.23688282],\n",
              "       [0.64391426]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    }
  ]
}